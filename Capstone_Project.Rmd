---
title: "Mile Stone Report"
author: "Juan Mari Sebastian Carino"
date: "February 3, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

## <b> Summary </b>

Our goal for this dataset is to make a predictive text application which suggests user text inputs whenever the user types something. This is similar to google search.  

The problem is: What form of analysis shall we do in predicting text? And, how? 


## <b> Gathering and Cleaning the Data </b>

I have manually downloaded the files and stored it in the Data Science Capstone folder. As part of my cleaning and exploring the data, I have loaded the following libraries to aid in my project.

```{r libraries}
library(tidyverse)
library(tidytext)
library(glue)
library(data.table)
library(readr)
library(quanteda)
```

Here, I have used converted the text data into a data frame. It is much easier to process data stored in a data frame rather than only in a character. 

```{r data}
blogs <- as_data_frame(readLines(con=file("~/Ambitions and Realities (Data Science, Career, Study Abroad)/Coursera/Data Science Specialization/Data Science Capstone/en_US.blogs.txt", open = "rt")))
colnames(blogs) <- "Blogs"

news <- as_data_frame(readLines(con=file("~/Ambitions and Realities (Data Science, Career, Study Abroad)/Coursera/Data Science Specialization/Data Science Capstone/en_US.news.txt", open = "rt")))
colnames(news) <- "News"

twitter <- as_data_frame(readLines(con=file("~/Ambitions and Realities (Data Science, Career, Study Abroad)/Coursera/Data Science Specialization/Data Science Capstone/en_US.twitter.txt", open = "rt")))
colnames(twitter) <- "Tweet"
```

I have used the sampling function of tidyverse package. I have gathered 10,000 random samples from each data. REPLACE = FALSE means that replacement is not allowed (i.e. no observations are duplicated in the samples). 

```{r sampling}
blogs <- sample_n(blogs, 10000, replace = FALSE)
news <- sample_n(news, 10000, replace = FALSE)
twitter <- sample_n(twitter, 10000, replace = FALSE)
```


Tokenization is a process of breaking sentences into individual words. I did the following:

1. I specified the unit for splitting (denoted by argument, what) to "word".
2. I removed the numbers, punctuations, symbols, separators, hyphens, and url. For twitter data, I also removed twitter characters @ and #. 
3. I removed the English stopwords. I set the padding to FALSE so that, there will be no empty string. 
4. I set the minimum character to 2 so that, no single letter will be tokenize. 

```{r tokenization}
# Tokenization
blogs_token <- tokens(blogs$Blogs, what = "word", remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_hyphens = TRUE, remove_url = TRUE) %>%
                  tokens_remove(stopwords("en"), padding = FALSE, min_nchar = 2)

news_token <- tokens(news$News, what = "word", remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_hyphens = TRUE, remove_url = TRUE) %>%
                  tokens_remove(stopwords("en"), padding = FALSE, min_nchar = 2)

twitter_token <- tokens(twitter$Tweet, what = "word", remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_hyphens = TRUE, remove_url = TRUE, remove_twitter = TRUE) %>% tokens_remove(stopwords("en"), padding = FALSE, min_nchar = 2)

```

After tokenization, I extracted the words from the token. In doing so, I made three sets: one-gram, two-gram, and three-gram. I have set all words to lowercase to be consistent and have specified stem to TRUE so that, R will get the root word. Once it is done, I stored it to document-feature matrix. 

```{Grams}
# One gram
blogs_t1 <- blogs_token %>%
              tokens_ngrams(n = 1) %>%
              dfm(tolower = TRUE, stem = TRUE)

news_t1 <- news_token %>%
              tokens_ngrams(n = 1) %>%
              dfm(tolower = TRUE, stem = TRUE)

twitter_t1 <- twitter_token %>%
              tokens_ngrams(n = 1) %>%
              dfm(tolower = TRUE, stem = TRUE)

# Two gram
blogs_t2 <- blogs_token %>%
              tokens_ngrams(n = 2) %>%
              dfm(tolower = TRUE, stem = TRUE)

news_t2 <- news_token %>%
              tokens_ngrams(n = 2) %>%
              dfm(tolower = TRUE, stem = TRUE)

twitter_t2 <- twitter_token %>%
              tokens_ngrams(n = 2) %>%
              dfm(tolower = TRUE, stem = TRUE)

# Three gram
blogs_t3 <- blogs_token %>%
              tokens_ngrams(n = 3) %>%
              dfm(tolower = TRUE, stem = TRUE)

news_t3 <- news_token %>%
              tokens_ngrams(n = 3) %>%
              dfm(tolower = TRUE, stem = TRUE)

twitter_t3 <- twitter_token %>%
              tokens_ngrams(n = 3) %>%
              dfm(tolower = TRUE, stem = TRUE)

```

## <b> Exploratory Data Analysis </b>

After processing, we can do the exploration. In our first exploration, we will extract the top 50 features for each gram. 

```{r features}
# Creating top 50 features function
top_n_features <- function(x, n){
    # x is a dfm
    # n is the number of features to extract (e.g. if top 50, then n = 50)
    features <- topfeatures(x, n) %>%
                  data.frame() %>%
                  rownames_to_column()
  }


# Extracting the top 50 words (one-gram)
topblogs_1g <- top_n_features(blogs_t1, 50)
topnews_1g <- top_n_features(news_t1, 50)
toptwitter_1g <- top_n_features(twitter_t1, 50)

# Showing the top 50 words
textplot_wordcloud(topblogs_1g)

```


```{r }
# Extracting the top 50 words (2-gram)
topblogs_2g <- top_n_features()
topnews_2g <- top_n_features()
toptwitter_2g <- top_n_features()

# Showing the top 50 words
top50_news_2g
top50_blogs_2g
top50_twitter_2g
```


```{r }
# Extracting the top 50 words (3-gram)
topblogs_3g <- top_n_features()
topnews_3g <- top_n_features()
toptwitter_3g <- top_n_features()

# Showing the top 50 words
top50_blogs_3g
top50_news_3g
top50_twitter_3g
```


## <b> Plans for Building the Model and App </b>
