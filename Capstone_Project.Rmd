---
title: "Mile Stone Report"
author: "Juan Mari Sebastian Carino"
date: "February 3, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

## <b> Summary </b>

Our goal for this dataset is to make a predictive text application which suggests user text inputs whenever the user types something. This is similar to google search.  

The problem is: What form of analysis shall we do in predicting text? And, how? 


## <b> Gathering and Cleaning the Data </b>

I have manually downloaded the files and stored it in the Data Science Capstone folder. As part of my cleaning and exploring the data, I have loaded the following libraries to aid in my project.

```{r libraries}
library(tidyverse)
library(tidytext)
library(glue)
library(data.table)
library(readr)
library(quanteda)
```

Here, I have used converted the text data into a data frame. It is much easier to process data stored in a data frame rather than only in a character. 

```{r data}
blogs <- as_data_frame(readLines(con=file("~/Ambitions and Realities (Data Science, Career, Study Abroad)/Coursera/Data Science Specialization/Data Science Capstone/en_US.blogs.txt", open = "rt")))
colnames(blogs) <- "Blogs"

news <- as_data_frame(readLines(con=file("~/Ambitions and Realities (Data Science, Career, Study Abroad)/Coursera/Data Science Specialization/Data Science Capstone/en_US.news.txt", open = "rt")))
colnames(news) <- "News"

twitter <- as_data_frame(readLines(con=file("~/Ambitions and Realities (Data Science, Career, Study Abroad)/Coursera/Data Science Specialization/Data Science Capstone/en_US.twitter.txt", open = "rt")))
colnames(twitter) <- "Tweet"
```

I have used the sampling function of tidyverse package. I have gathered 10,000 random samples from each data. REPLACE = FALSE means that replacement is not allowed (i.e. no observations are duplicated in the samples). 

```{r sampling}
blogs <- sample_n(blogs, 10000, replace = FALSE)
news <- sample_n(news, 10000, replace = FALSE)
twitter <- sample_n(twitter, 10000, replace = FALSE)
```



```{r tokenization}
# Tokenization
blogs_token <- tokens(blogs$Blogs, what = "word", remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_hyphens = TRUE, remove_url = TRUE) %>%
                  tokens_remove(stopwords("english"), padding = FALSE, min_nchar = 2)

news_token <- tokens(news$News, what = "word", remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_hyphens = TRUE, remove_url = TRUE) %>%
                  tokens_remove(stopwords("english"), padding = FALSE, min_nchar = 2)

twitter_token <- tokens(twitter$Tweet, what = "word", remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_hyphens = TRUE, remove_url = TRUE, remove_twitter = TRUE) %>%
                  tokens_remove(stopwords("english"), padding = FALSE, min_nchar = 2)

# One gram
blogs_t1 <- blogs_token %>%
              tokens_ngrams(n = 1) %>%
              dfm(tolower = TRUE, stem = TRUE)

news_t1 <- news_token %>%
              tokens_ngrams(n = 1) %>%
              dfm(tolower = TRUE, stem = TRUE)

twitter_t1 <- twitter_token %>%
              tokens_ngrams(n = 1) %>%
              dfm(tolower = TRUE, stem = TRUE)

# Two gram
blogs_t2 <- blogs_token %>%
              tokens_ngrams(n = 2) %>%
              dfm(tolower = TRUE, stem = TRUE)

news_t2 <- news_token %>%
              tokens_ngrams(n = 2) %>%
              dfm(tolower = TRUE, stem = TRUE)

twitter_t2 <- twitter_token %>%
              tokens_ngrams(n = 2) %>%
              dfm(tolower = TRUE, stem = TRUE)

# Three gram
blogs_t3 <- blogs_token %>%
              tokens_ngrams(n = 3) %>%
              dfm(tolower = TRUE, stem = TRUE)

news_t3 <- news_token %>%
              tokens_ngrams(n = 3) %>%
              dfm(tolower = TRUE, stem = TRUE)

twitter_t3 <- twitter_token %>%
              tokens_ngrams(n = 3) %>%
              dfm(tolower = TRUE, stem = TRUE)

```

## <b> Exploratory Data Analysis </b>

After pre-processing, the top features of one-gram or words are as follows:

```{r }
# Extracting the top 50 words (1-gram)
top50_news_1g <- topfeatures(news_t1, n = 50) %>%
          data.frame() %>%
          rownames_to_column()

top50_blogs_1g <- topfeatures(blogs_t1, n = 50) %>%
               data.frame() %>%
               rownames_to_column()

top50_twitter_1g <- topfeatures(twitter_t1, n = 50) %>%
                 data.frame() %>%
                 rownames_to_column()

# Showing the top 50 words
top50_news_1g
top50_blogs_1g
top50_twitter_1g
```

```{r }
# Extracting the top 50 words (2-gram)
top50_news_2g <- topfeatures(news_t2, n = 50) %>%
                  data.frame() %>%
                  rownames_to_column()

top50_blogs_2g <- topfeatures(blogs_t2, n = 50) %>%
                  data.frame() %>%
                  rownames_to_column()

top50_twitter_2g <- topfeatures(twitter_t2, n = 50) %>%
                    data.frame() %>%
                    rownames_to_column()

# Showing the top 50 words
top50_news_2g
top50_blogs_2g
top50_twitter_2g
```


```{r }
# Extracting the top 50 words (3-gram)
top50_news_3g <- topfeatures(news_t3, n = 50) %>%
                 data.frame() %>%
                 rownames_to_column()

top50_blogs_3g <- topfeatures(blogs_t3, n = 50) %>%
                  data.frame() %>%
                  rownames_to_column()

top50_twitter_3g <- topfeatures(twitter_t3, n = 50) %>%
                    data.frame() %>%
                    rownames_to_column()

# Showing the top 50 words
top50_blogs_3g
top50_news_3g
top50_twitter_3g
```

